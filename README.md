# Operation Ditwah â€“ Crisis Intelligence Pipeline

**Scenario:** Post-Cyclone Ditwah Relief (Sri Lanka) â€“ December 2025

---

## ğŸ“‹ Project Overview

This project implements a **Crisis Intelligence Pipeline** designed to assist the **Disaster Management Center (DMC)** during a fictional cyclone crisis.

The system:

* Filters noise from social media
* Validates genuine rescue requests
* Optimizes logistics for limited resources
* Converts unstructured news feeds into actionable databases

The pipeline handles **five core tasks**:

* **Reliability (The Contract):** Distinguishes real SOS calls from news noise using *Few-Shot Prompting*.
* **Safety (Stability Experiment):** Tests system stability against hallucinations by varying model temperature (*Safe Mode vs. Chaos Mode*).
* **Strategic Planning (Logistics Commander):** Plans complex logistics for limited resources using *Chain of Thought (CoT)* and *Tree of Thoughts (ToT)* reasoning.
* **Efficiency (Budget Keeper):** Rejects or summarizes token-heavy spam to save API costs.
* **Scalability (News Feed Pipeline):** Processes a live news feed into a structured Excel database using *Pydantic* and *Pandas*.

---

## ğŸ› ï¸ Prerequisites

* Python **3.8+**
* **Groq API Key** (Required for Llama 3 model integration)

---

## ğŸ“¦ Installation & Setup

### Project Setup

Ensure your project folder follows the structure defined below.

### Create Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Mac / Linux
source venv/bin/activate
```

### Install Dependencies

```bash
pip install -r requirements.txt
```

### Environment Configuration

Create a `.env` file in the root directory and add your Groq API key:

```ini
GROQ_API_KEY=gsk_your_api_key_here
```

---

## ğŸ“‚ Project Structure

```plaintext
operation_ditwah/
â”‚
â”œâ”€â”€ data/                       # Input datasets
â”‚   â”œâ”€â”€ sample_messages.txt     # Mixed SOS / News messages
â”‚   â”œâ”€â”€ incidents.txt           # Critical incidents for logistics
â”‚   â”œâ”€â”€ scenarios.txt           # Complex scenarios for stress testing
â”‚   â””â”€â”€ news_feed.txt           # Raw news ticker feed
â”‚
â”œâ”€â”€ output/                     # Generated results
â”‚   â”œâ”€â”€ classified_messages.xlsx # Output from Part 1
â”‚   â””â”€â”€ flood_report.xlsx        # Output from Part 5
â”‚
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ classifier.py     # Few-Shot Learning Classifier
â”‚   â”œâ”€â”€ stability.py      # Temperature / Hallucination Test
â”‚   â”œâ”€â”€ logistics.py      # CoT & ToT Logic Planner
â”‚   â”œâ”€â”€ budget.py         # Token Limiter & Summarizer
â”‚   â””â”€â”€ newsfeed.py       # JSON Extraction Pipeline
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ token_utils.py          # Helper for counting tokens
â”‚
â”œâ”€â”€ requirements.txt            # Project dependencies
â””â”€â”€ README.md                   # Project documentation
```

---

## ğŸš€ Usage Guide

### Part 1: The "Contract" (Classifier)

Classifies incoming messages into **Rescue**, **Supply**, **Info**, or **Other** categories.

```bash
python src/classifier.py
```

* **Goal:** Engineer a reliable classification prompt using few-shot examples
* **Output:** `output/classified_messages.xlsx`

---

### Part 2: The Stability Experiment

Runs a stress test on the AI model by comparing:

* **Safe Mode:** Temperature = 0.0
* **Chaos Mode:** Temperature = 1.0

```bash
python src/stability.py
```

* **Goal:** Observe how high temperature causes the model to drift or hallucinate resources

---

### Part 3: The Logistics Commander

Uses **Chain of Thought (CoT)** to score incident urgency and **Tree of Thoughts (ToT)** to determine the optimal rescue route for a single boat.

```bash
python src/logistics.py
```

* **Goal:** Maximize total priority score saved within the shortest time

---

### Part 4: The "Budget Keeper"

Implements token economics. Messages exceeding **150 tokens** are flagged and summarized before processing.

```bash
python src/budget.py
```

* **Goal:** Print `BLOCKED` / `TRUNCATED` for spam inputs

---

### Part 5: The "News Feed" Pipeline

Converts the unstructured `news_feed.txt` into a structured Excel database using **Pydantic** for strict schema validation.

```bash
python src/newsfeed.py
```

* **Extracted Fields:**

  * `district`
  * `flood_level_meters`
  * `victim_count`
  * `main_need`
  * `status`

* **Output:** `output/flood_report.xlsx`

---
